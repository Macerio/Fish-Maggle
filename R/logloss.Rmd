---
title: "Explore the logloss function"
author: "Jordan Trémoureux"
date: "November 30, 2016"
output: 
  html_document:
    keep_md: true
    fig_height: 4
    fig_width: 9
    highlight: kate
    theme: spacelab
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = FALSE,
  comment = "#>",
fig.retina = 2,
  fig.path = "md_figs/fig-"
)
```

```{r, message = F, warning = F}
# Loading packages
pkgs = c("ggplot2", "data.table")
inst = lapply(pkgs, library, character.only = TRUE)
```

----------

# Goal
My goal here is to describe the [multi-class logarithmic loss](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/details/evaluation) (aka. logloss) function that is used to evaluate the submissions.  

# The loggloss function

## What is this function ?

$$
\text{logloss} = -\dfrac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{M} y_{ij} \log(p_{ij})
$$
  
- $N$ is the number of image in the test set, so $N=1000$.  
- $i$ is an image in the test set.  
- $M$ is the number of image class labels, so $M=8$ because class labels are :
```{r, message = F, warning = F}
labels <- c('ALB'=1, 'BET'=2, 'DOL'=3, 'LAG'=4, 'NoF'=5, 'OTHER'=6, 'SHARK'=7,'YFT'=8)
lab <- c('ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK','YFT')
```
- $j$ is one class label.  
- $\log$ is the natural logarithm  
  
- $y_{ij} = 1$ if the image $i$ belongs to the class $j$, and $y_{ij} = 0$ otherwise.    
- $p_{i} \in [0,1]^8$ are your 8 probabilities $p_{ij}$ for the image $i$. With $p_{ij}$ the probability that the image $i$ belongs to the class $j$.  

For example, if your submission looks like this :

```{r}
pred <- data.frame(image = c("img_00001.jpg", "img_00002.jpg"),
                   ALB = c(1, 0.6), BET = c(0, 0.2), DOL = c(0,1.2), LAG = c(0,0),
                   NoF = c(0,0), OTHER = c(0,0), SHARK = c(0,0), YFT = c(0,0))
pred
```

This means :  
- $p_{11} = 1$, $p_{12} = 0$, ..., $p_{18} = 0$   
- $p_{21} = 0.6$, $p_{22} = 0.2$, $p_{23} = 1.2$, $p_{24} = 0$, ..., $p_{28} = 0$   
- ...

## Some warnings 

Wait!! Some of your probabilities are $> 1$!  

You're right but, in fact, *the submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum).*  

Then those probabilities became :
```{r}
lab <- 2:9
pred[,lab] <- data.frame(t(apply(pred[,lab], 1, function(x) x/sum(x))))
```

Wait!! Some probabilities are equal to 0, and $\log (0)$...

You're right but, in fact, *In order to avoid the extremes of the log function, predicted probabilities are replaced with*
$$
\max(\min(p,1−10^{−15}),10^{−15})
$$

```{r}
maxmin <- function(predicted, eps=1e-15) {
  return(max(min(predicted, 1-eps), eps))
}
```

That's why, this submission will be transformed to :
```{r}
pred2 <- pred
lab = 2:9
pred2[,lab] <- data.frame(t(apply(pred2[,lab], 1, function(x) sapply(x, maxmin))))
pred2
```

This means :  
- $p_{11} = 1-10^{-15}, p_{12} = 10^{-15}, ..., p_{18} = 10^{-15}$  
- $p_{21} = 0.3, p_{22} = 0.1, p_{23} = 0.6, p_{24} = 10^{-15}, ..., p_{28} = 10^{-15}$  
- ...

Wait!! With this transformation, the sum $\sum_{j=1}^{9} p_{ij} \neq 1$!  
Yes you're right.
```{r}
apply(pred2[,lab], 1, sum) == c(1, 1)
```

In fact this is pretty close, and I think this will not change anything. Or maybe the probabilities are rescale a second time.

```{r, eval=FALSE, include=FALSE}
pred3 <- pred2
pred3[,lab] <- data.frame(t(apply(pred3[,lab], 1, function(x) x/sum(x))))

apply(pred3[,lab], 1, sum) == c(1, 1)
```

# Some classifiers
## The perfect classifier - Score = 0

Let's see the images in the test set 
```{r}
set.seed(123)
test_folder <- '../../input/test_stg1'
n_test <- length(list.files(test_folder))
n_test
```
There is `r n_test` images in the test data set.

So what is the score of the perfect classifier ?   
The best classifier will predict $p_{ij}=1$ when the image $i$ belongs to the class $j$, ie when $y_{ij}=1$, and 0 otherwise (when)  
$$
\text{logloss} = -\dfrac{1}{100} \sum_{i=1}^{100} 1 \times \log(1) = 0
$$
That's why, the more you are closed to 0, the best rank you have in the [leaderboard](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/leaderboard).

<small>
In fact, getting exactly 0 is impossible, because probabilities can't be equal to 1.  
The best score you can have should be something like $-\dfrac{1}{100} \sum_{i=1}^{100} 1 \times \log(1-10^{-15}) = -\log(1-10^{-15}) \simeq + 9.992007 10^{-16} \simeq 0$  
</small>

## A very naive classifier - Score = 2.07944
Since there are 8 class labels, why not just set all probabilities to $\dfrac{1}{8}$?  
This seems fun to me, let's do this!

```{r}
subm0 <- data.table(image = list.files(test_folder),
                ALB = 1/8, BET = 1/8, DOL = 1/8, LAG = 1/8,
                NoF = 1/8, OTHER = 1/8, SHARK = 1/8, YFT = 1/8)
write.csv(subm0, file = "../../output/subm0.csv", row.names = FALSE)
# Your submission scored 2.07944, 448/486.. not really good
```


## A less naive classifier - Score = 1.64611
Maybe $\dfrac{1}{8}$ for everyone is too naive. Let's see how is our training data set.

```{r}
set.seed(123)
train_folder <- '../../input/train'
print(list.files(train_folder))
tmp <- lapply(list.files(train_folder), function(x) list.files(paste0(train_folder, '/', x)))
train_files <- data.table(label = rep(list.files(train_folder), sapply(tmp, length)),
             image = unlist(tmp))
train_files
```

```{r ggplot}
n_train <- nrow(train_files) # 3777

ggplot(train_files, aes(x=label, fill=label)) + 
      geom_bar(colour="black") +
      geom_label(stat='count', aes(label=..count..), show.legend = FALSE) +
      ggtitle("Count of files for each label")
```

There are a lot more of **ALB** (1719, Albacore tuna) in the dataset than the others. There are few **LAG** (67, Opah).   
What we can do, is to set the probabilities to the frequency of each class label in the dataset.

```{r}
label_DT <- train_files[, .N, label][, freq:=N/n_train]
setkey(label_DT, label)
label_DT
```

```{r}
subm1 <- data.table(image = list.files(test_folder),
                ALB = label_DT["ALB",freq], BET = label_DT["BET",freq], DOL = label_DT["DOL",freq], LAG = label_DT["LAG",freq],
                NoF = label_DT["NoF",freq], OTHER = label_DT["OTHER",freq], SHARK = label_DT["SHARK",freq], YFT = label_DT["YFT",freq])
write.csv(subm1, file = "../../output/subm1.csv", row.names = FALSE)

# Your submission scored 1.64611
```

In fact, this solution really looks like the Sample Submission Benchmark in the file *sample_submission_stg1.csv*.

## The benchmark classifier - Score = 1.61364
```{r}
subm <- data.table(read.csv("../../input/sample_submission_stg1.csv"))
subm[1,lab,with=F]
subm1[1,lab,with=F]
```

This benchmark can be find in the [leaderboard](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/leaderboard), with a score of 1.61364.   

I don't know why the benchmark is not *subm1*.. Does anyone know ?  
Maybe in the benchmark, the probabilities are the frequencies in the test set ? :D.

## A real classifier : Fishy Keras #1 - Score = 1.29546

Thanks to the popular kernel [Fishy Keras [LB: 1.25267]](https://www.kaggle.com/zfturbo/the-nature-conservancy-fisheries-monitoring/fishy-keras-lb-1-25267) by [ZFTurbo](https://www.kaggle.com/zfturbo) you can get the [output](https://www.kaggle.com/zfturbo/the-nature-conservancy-fisheries-monitoring/fishy-keras-lb-1-25267/output) for a real classifier.


```{r}
fileUrl2 <- "https://www.kaggle.io/svf/445238/40dacb2a0eb96787be3e4373d253071e/submission_loss_loss_0.779022299035_folds_3_ep_30_folds_3_2016-11-14-15-31.csv"
download.file(fileUrl2, method = "curl", destfile = "../../output/subm2.csv")
subm2 <- data.table(read.csv("../../output/subm2.csv"))
#write.csv(subm2, file = "../../output/subm2.csv", row.names = FALSE)
subm2
# Your submission scored 1.29546
```

## A real classifier : Fishy Keras #2 - Score = 1.14379

[pankajsharma](https://www.kaggle.com/pankajvshrma) and his team **The Proud Team**, improved the Fishy Keras classifier.


```{r}
fileUrl3 <- "https://www.kaggle.io/svf/464662/c3db400681ddd755421841d4f08dfa15/submission_loss_loss_0.61935684931_folds_3_ep_15_fl_96_folds_3_2016-11-24-10-39.csv"
download.file(fileUrl3, method = "curl", destfile = "../../output/subm3.csv")
subm3 <- data.table(read.csv("../../output/subm3.csv"))

# Your submission scored 1.14379
```

## Aggregate two classifier ? #1 Score = ?

```{r}
boost1 <- 0.9*subm2[,lab,with=F] + 0.1*subm1[,lab,with=F]
boost1$image <- subm2$image
write.csv(boost1, file = "../../output/boost1.csv", row.names = FALSE)

# Your submission scored ?
```

## Aggregate two classifier ? #2 Score = 1.15792

```{r}
boost2 <- 0.9*subm3[,lab,with=F] + 0.1*subm1[,lab,with=F]
boost2$image <- subm3$image
write.csv(boost2, file = "../../output/boost2.csv", row.names = FALSE)

# Your submission scored 1.15792
```


## Aggregate two classifier ? #3 Score = ?

```{r}
boost3 <- 0.6*subm3[,lab,with=F] + 0.4*subm2[,lab,with=F]
boost3$image <- subm3$image
write.csv(boost3, file = "../../output/boost3.csv", row.names = FALSE)

# Your submission scored 1.15792
```


